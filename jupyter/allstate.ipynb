{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cont_features = [\"cont\"+str(i+1) for i in range(14)]\n",
    "cat_features = [\"cat\"+str(i+1) for i in range(116)]\n",
    "target = [\"loss\"]\n",
    "\n",
    "#unique_categories = {c:np.unique(list(train_data[c])+list(test_data[c])) for c in cat_features}\n",
    "unique_categories = {x[0]:x[1].split() for x in pd.read_csv(\"categories.csv\").as_matrix()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoder\n",
    "def encoder(df, cat_features, cat_dict):\n",
    "    tr_data = df.copy()\n",
    "\n",
    "    new_cat_features = []\n",
    "\n",
    "    for col in cat_features:\n",
    "        uniq = cat_dict[col]\n",
    "        if len(uniq) == 2:\n",
    "            tr_data[col+'_0'] = np.array(map(int, tr_data[col]==uniq[0]))\n",
    "            new_cat_features += [col+'_0']\n",
    "        else:\n",
    "            for j, v in enumerate(uniq):\n",
    "                tr_data[col+'_'+str(j)] = np.array(map(int, tr_data[col]==uniq[j]))\n",
    "                new_cat_features += [col+'_'+str(j)]\n",
    "    return tr_data.copy(), new_cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  1  done\n",
      "step  2  done\n",
      "step  3  done\n",
      "step  4  done\n",
      "step  5  done\n",
      "step  6  done\n",
      "step  7  done\n",
      "step  8  done\n",
      "step  9  done\n",
      "step  10  done\n",
      "step  11  done\n",
      "step  12  done\n",
      "step  13  done\n",
      "step  14  done\n",
      "step  15  done\n",
      "step  16  done\n",
      "step  17  done\n",
      "step  18  done\n",
      "step  19  done\n",
      "step  20  done\n",
      "step  21  done\n",
      "step  22  done\n",
      "step  23  done\n",
      "step  24  done\n",
      "step  25  done\n",
      "step  26  done\n",
      "step  27  done\n",
      "step  28  done\n",
      "step  29  done\n",
      "step  30  done\n",
      "step  31  done\n",
      "step  32  done\n",
      "step  33  done\n",
      "step  34  done\n",
      "step  35  done\n",
      "step  36  done\n",
      "step  37  done\n",
      "step  38  done\n",
      "step  39  done\n",
      "step  40  done\n",
      "step  41  done\n",
      "step  42  done\n",
      "step  43  done\n",
      "step  44  done\n",
      "step  45  done\n",
      "step  46  done\n",
      "step  47  done\n",
      "step  48  done\n",
      "step  49  done\n",
      "step  50  done\n",
      "step  51  done\n",
      "step  52  done\n",
      "step  53  done\n",
      "step  54  done\n",
      "step  55  done\n",
      "step  56  done\n",
      "step  57  done\n",
      "step  58  done\n",
      "step  59  done\n",
      "step  60  done\n",
      "step  61  done\n",
      "step  62  done\n",
      "step  63  done\n",
      "step  64  done\n",
      "step  65  done\n",
      "step  66  done\n",
      "step  67  done\n",
      "step  68  done\n",
      "step  69  done\n",
      "step  70  done\n",
      "step  71  done\n",
      "step  72  done\n",
      "step  73  done\n",
      "step  74  done\n",
      "step  75  done\n",
      "step  76  done\n"
     ]
    }
   ],
   "source": [
    "reader = pd.read_csv(\"train.csv\", chunksize=2500)\n",
    "\n",
    "models = []\n",
    "step = 0\n",
    "\n",
    "for train_data in reader:\n",
    "    \n",
    "    step += 1\n",
    "    \n",
    "    tr_data, new_cat_features = encoder(train_data, cat_features, unique_categories)\n",
    "    #tr_data = tr_data.reindex(np.random.permutation(tr_data.index))\n",
    "    #train_data, val_data = tr_data[:1000], tr_data[1000:2000]\n",
    "\n",
    "    X_train, y_train = tr_data[cont_features+new_cat_features].as_matrix(), tr_data[target].as_matrix().transpose()[0]\n",
    "    #X_val, y_val = val_data[cont_features+new_cat_features].as_matrix(), val_data[target].as_matrix().transpose()[0]\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=1, criterion=\"mae\", max_depth=3)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    models += [model]\n",
    "    print \"step \",step,\" done\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  1  done\n",
      "step  2  done\n",
      "step  3  done\n",
      "step  4  done\n",
      "step  5  done\n",
      "step  6  done\n",
      "step  7  done\n",
      "step  8  done\n",
      "step  9  done\n",
      "step  10  done\n",
      "step  11  done\n",
      "step  12  done\n",
      "step  13  done\n",
      "step  14  done\n",
      "step  15  done\n",
      "step  16  done\n",
      "step  17  done\n",
      "step  18  done\n",
      "step  19  done\n",
      "step  20  done\n",
      "step  21  done\n",
      "step  22  done\n",
      "step  23  done\n",
      "step  24  done\n",
      "step  25  done\n",
      "step  26  done\n",
      "step  27  done\n",
      "step  28  done\n",
      "step  29  done\n",
      "step  30  done\n",
      "step  31  done\n",
      "step  32  done\n",
      "step  33  done\n",
      "step  34  done\n",
      "step  35  done\n",
      "step  36  done\n",
      "step  37  done\n",
      "step  38  done\n",
      "step  39  done\n",
      "step  40  done\n",
      "step  41  done\n",
      "step  42  done\n",
      "step  43  done\n",
      "step  44  done\n",
      "step  45  done\n",
      "step  46  done\n",
      "step  47  done\n",
      "step  48  done\n",
      "step  49  done\n",
      "step  50  done\n",
      "step  51  done\n",
      "step  52  done\n",
      "step  53  done\n",
      "step  54  done\n",
      "step  55  done\n",
      "step  56  done\n",
      "step  57  done\n",
      "step  58  done\n",
      "step  59  done\n",
      "step  60  done\n",
      "step  61  done\n",
      "step  62  done\n",
      "step  63  done\n",
      "step  64  done\n",
      "step  65  done\n",
      "step  66  done\n",
      "step  67  done\n",
      "step  68  done\n",
      "step  69  done\n",
      "step  70  done\n",
      "step  71  done\n",
      "step  72  done\n",
      "step  73  done\n",
      "step  74  done\n",
      "step  75  done\n",
      "step  76  done\n",
      "1409.90794357\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "train_data, ncf = encoder(train_data, cat_features, unique_categories)\n",
    "\n",
    "X_train = train_data[cont_features+ncf].as_matrix()\n",
    "y_train = train_data[target].as_matrix().transpose()[0]\n",
    "\n",
    "pred = np.zeros(train_data.shape[0])\n",
    "\n",
    "for step, model in enumerate(models):\n",
    "    pred += model.predict(X_train)\n",
    "    print \"step \",step+1,\" done\"\n",
    "pred = pred / float(len(models))\n",
    "\n",
    "pred = np.array(map(lambda x: max(x,0.0), pred))\n",
    "print np.mean(np.abs(pred - train_data[target].as_matrix().transpose()[0]))\n",
    "#answer = pd.DataFrame(zip(test_data[\"id\"], pred), columns=[\"id\", \"loss\"])\n",
    "#answer.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_data, ncf = encoder(test_data, cat_features, unique_categories)\n",
    "\n",
    "X_test = test_data[cont_features+ncf].as_matrix()\n",
    "pred = np.zeros(test_data.shape[0])\n",
    "\n",
    "for model in models:\n",
    "    pred += model.predict(X_test)\n",
    "pred = pred / float(len(models))\n",
    "\n",
    "pred = np.array(map(lambda x: max(x,0.0), pred))\n",
    "answer = pd.DataFrame(zip(test_data[\"id\"], pred), columns=[\"id\", \"loss\"])\n",
    "answer.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvs_train, cvs_val = [], []\n",
    "rang = np.arange(1,11,1)\n",
    "\n",
    "for n in rang:\n",
    "    model = BaggingRegressor(\n",
    "        RandomForestRegressor(criterion='mae', n_estimators=10, max_features=\"log2\"), \n",
    "        n_estimators=n)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_train)\n",
    "    cvs_train += [np.mean(np.abs(y_train-pred))]\n",
    "    \n",
    "    pred = model.predict(X_val)\n",
    "    cvs_val += [np.mean(np.abs(y_val-pred))]\n",
    "    \n",
    "    #cvs += [np.mean(cross_val_score(model, X, y, scoring=\"neg_mean_absolute_error\"))]\n",
    "    print n\n",
    "\n",
    "#rang = range(1,122,5)\n",
    "plt.plot(rang, cvs_train, rang, cvs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvs_train, cvs_val = [], []\n",
    "#tr1_data = tr_data.copy()\n",
    "rang = range(10)\n",
    "\n",
    "for i in rang:\n",
    "    tr1_data = tr1_data.reindex(np.random.permutation(tr1_data.index))\n",
    "    train_data, val_data = tr1_data[:1000], tr1_data[1000:2000]\n",
    "    X_train, y_train = train_data[cont_features+new_cat_features].as_matrix(), func(train_data[target].as_matrix().transpose()[0])\n",
    "    X_val, y_val = val_data[cont_features+new_cat_features].as_matrix(), func(val_data[target].as_matrix().transpose()[0])\n",
    "    \n",
    "    #model = BaggingRegressor(RandomForestRegressor(criterion='mae', n_estimators=10, max_features=\"log2\"), n_estimators=10)\n",
    "    model = GradientBoostingRegressor(loss='lad', learning_rate=0.3, n_estimators=100)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred = invfunc(model.predict(X_train))\n",
    "    cvs_train += [np.mean(np.abs(y_train-pred))]\n",
    "    \n",
    "    pred = invfunc(model.predict(X_val))\n",
    "    cvs_val += [np.mean(np.abs(y_val-pred))]\n",
    "    \n",
    "    print i\n",
    "    \n",
    "plt.plot(rang, cvs_train, rang, cvs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_data, ncf = encoder(test_data, cat_features, unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X_test = test_data[cont_features+ncf].as_matrix()\n",
    "pred = model.predict(X_test)\n",
    "pred = np.array(map(lambda x: max(x,0.0), pred))\n",
    "answer = pd.DataFrame(zip(test_data[\"id\"], pred), columns=[\"id\", \"loss\"])\n",
    "answer.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelX_train = np.array(map(lambda x: x.predict(X_train), models)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='lad', max_depth=11,\n",
       "             max_features='log2', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=111, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalmodel = GradientBoostingRegressor(loss='lad', max_depth=11, max_features=\"log2\", n_estimators=111)\n",
    "finalmodel.fit(modelX_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = finalmodel.predict(modelX_train)\n",
    "print np.mean(np.abs(pred - y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1303.8290255\n"
     ]
    }
   ],
   "source": [
    "print -np.mean(cross_val_score(\n",
    "                BaggingRegressor(\n",
    "                GradientBoostingRegressor(loss='lad', max_depth=3, max_features=\"log2\", n_estimators=100), \n",
    "                n_estimators=10), \n",
    "                          modelX_train, y_train, \n",
    "                          scoring=\"neg_mean_absolute_error\", cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_data, ncf = encoder(test_data, cat_features, unique_categories)\n",
    "\n",
    "X_test = test_data[cont_features+ncf].as_matrix()\n",
    "modelX_test = np.array(map(lambda x: x.predict(X_test), models)).transpose()\n",
    "pred = finalmodel.predict(modelX_test)\n",
    "pred = np.array(map(lambda x: max(x,0.0), pred))\n",
    "answer = pd.DataFrame(zip(test_data[\"id\"], pred), columns=[\"id\", \"loss\"])\n",
    "answer.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
